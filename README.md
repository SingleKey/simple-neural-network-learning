# simple-neural-network-learning

0基础无微积分前置知识，学习简单神经网络的原理和使用java代码复刻

## 一、项目背景知识

我学习到了神经网络的前向传播和反向传播，现将相关代码和学习过程，以及相关笔记和感悟记录，希望通过记录所学知识，未来忘记的时候可以复习，也希望可以帮助更多对机器学习和神经网络感兴趣，但是又没有相关知识的朋友。

## 二、初步认识神经网络时的感受

在我印象里，机器学习只有本科研究生数学水平的人才能学会的，我作为一个初中数学水平的人，一直对神经网络很感兴趣，但是我不会高等数学，因为我连高中都没有读过，就连高中数学都没有接触过，更别说高等数学，同时因为不了解相关前置知识，又不会英语，导致国内的视频看不懂，国外的英语教材也看不懂......

转折点在最近(2025年12月19日)，我在bilibili刷短视频的时候，刷到了[【【AI入门】【用Excel实现神经网络】】](https://b23.tv/yhO6559) 。因为看标题是使用Excel实现神经网络，所以我想看看是怎么回事，但是发现这也还是一个教程，带着心中对机器学习知识的渴望和无法学习的不甘，我想也没想的在评论区吐槽着零基础学不会，但是作者老师回复我，说这就是面向零基础的系列课程，而且还有Excel实现案例，我就抱着玩玩的心态，到github上下载了Excel案例 [zeroai](https://github.com/jiangsongyi0204/zeroai)，在电脑上运行。

原来神经网络是长这个样子的，这个神经网络是一个吃豆人的操作网络，九宫格里面，红色代表吃豆人的位置，绿色代表豆子的位置，豆子出现时，红色格子里的箭头代表要向哪个方向前进才能以最少步数吃到最多豆子，右侧的数据格子是权重数据，最右侧的是训练数据，一行代表一条数据，左侧的P1到P8代表输入的数据，右侧A1到A4代表这条数据的答案：

![Excel神经网络](./images/net01.png "Excel神经网络")

点击初始化神经网络，点击批量训练，然后点点点，点十几下，点的过程中发现线条变得粗细不一，然后再点测试按钮，就可以发现这个神经网络很神奇，通过某种计算，就可以让吃豆人吃到豆子：

![Excel神经网络2](./images/net02.png "Excel神经网络2")

## 三、初识神经网络

这和我一开始我想的一样，就是众所周知的只有三层节点的网络，等我尝试复刻的时候，我发现这个如果只存在三层节点，根本无法做任何计算，因为计算是需要等式，而这三层节点，第一层节点存的是输入的参数，中间层的节点和最后一层节点存储的是计算结果，所以其中肯定存在别的计算参数，通过与输入参数进行计算才能得出结果，然后经过两轮计算才能得到最终的结果。

![Excel神经网络3](./images/net03.png "Excel神经网络3")

但是在右侧，我看到神经网络中还包含两个权重参数和两个偏置参数：

![Excel神经网络4](./images/net04.png "Excel神经网络4")

我想到了，肯定是这两个参数参与计算，才能得出结果，所以实际上并非三层节点，而是3层节点+2层权重+2层偏置=7层节点，通过某种形式的计算，才得出来最终的结果，所以，刚刚那个红色和蓝色的线条，其实就是权重，所以线条会在训练后变粗：

![Excel神经网络5](./images/net05.png "Excel神经网络5")

上面是一个简化简化过的两层神经网络之间的计算过程示意图，图中的A代表输入层，W代表输出层，B代表偏置层，O代表输出层，作者老师说，通过以下公式计算出结果：
~~~text
输入层数据 * 权重1 + 偏置1 = 计算结果
input * weight1 + bias1 = temp_result

使用sigmoid函数计算此结果，得到隐藏层数据
sigmoid(temp_result) = hidden

隐藏层数据 * 权重2 + 偏置2 = 计算结果2
hidden * weight2 + bias2 = temp_result2

最后使用softmax函数将计算结果2的数据转化为总和为1(100%)的数，然后放在输出层
softmax(temp_result2) = output
~~~

> 这里的sigmoid函数和softmax函数并没写出函数的内容，相当于占位符，表示其中的参数经过某种计算以后，得到等号右边的结果，函数的具体内容在后面数学知识部分有讲解

如果要通俗的理解这个公式，就可以将这个公式比作一个开空调的过程，把`A`比作要调节的温度，`W`需要调整多少度，`B`至少要调节的度数，然后`temp_result`最终调整完成的温度。但是光调温度还不行，还要知道这个温度到底是冷还是热，这就需要心里面有一条判断温度的标准线，通过人体去体验这个温度是否合适，而这个判断的过程就是激活函数（Logistic）。

开空调的神经网络结果，会被softmax将最终的计算结果转化为概率。比如设置了五个心里答案，很热，有点热，正常，凉快，有点冷，通过转化概率，五个预期答案出现的概率总和是100%。比如很热12%，有点热38%，正常20%，凉快16%，有点冷14%，那么最大概率就是有点热。接下来就看哪个温度符合实际情况，如果温度高于或者低于自己的心理预期，就要往正确方向调整空调温度，神经网络也是走的这个过程。

因为神经网络无法一开始就判断空调温度的高低，就需要训练，输入当前空调温度，外界温度，以及其他环境因素，然后输出结果，根据正确答案和输出结果，调整空调温度，如果每次调1摄氏度，那调整的学习率就是1/(空调最高温 - 空调最低温)，让空调温度和我们觉得舒服的温度的差距变小，因为事先不知道最合理的温度是多少，所以需要小步快走，多次调整，直到达到我们想要的温度为止，这就是学习过程。

由多个计算过程的叠加，就形成了一个完整的针对多个指标的处理和判断的程序，比如同时输入温度，湿度，去判断体感温度是冷是热，空气是否干燥，是否潮湿等，这就是神经网络的原理，判断的方法有很多，这里使用`sigmoid`函数作为判断标准，通过有时候通过初次判断和再判断，会让结果更加的准确，所以会设计成多层的神经网络。

## 四、前向传播和反向传播

开空调的时候，就是前向传播的过程，也是预测温度的过程，然后觉得冷，主动去调整空调温度，就是学习的过程，就是反向传播过程。

经过对神经网络的初步认识，就可以直接根据现有的公式进行计算，我也是在学习后得出结论，数学公式并没有那么高深莫测，只要知道符号含义，计算规则，就可以看懂这个数学公式，看不懂公式，是因为从来没有人告诉我们公式里的符号是什么意思，如何去计算。

学习神经网络需要的几个知识点都可以跟生活中常见的事物联系起来，并不难理解，这些知识点，分别是微积分、链式法则、计算图、还有一点矩阵向量的知识，这些知识的名字看起来让人感到非常恐惧，但是请不要害怕，觉得困难，只是我们对数学不够了解，以及对数学知识的敬畏之心。


### 4.1、公式和数学知识讲解

微积分描述的是一个事物变化过程中的量，这个量不是固定的，是一直在变化的，像方程，但是方程有固定量，而微积分没固定量，没有固定量，意味着你可以带入任何数字到式子中，然后计算出结果，这些结果可以连接成一条线，这条线就是数学中的函数图。

#### 4.1.1 函数

我们可以给这个式子命名，比如叫`sigmoid`函数，`sigmoid`函数已经被数学家定义好了，就像三角函数或者其他的什么函数一样，只需要理解其中符号的原理，然后拿来用就好：

![sigmoid激活函数方程](./images/sigmoid-function.png "sigmoid激活函数方程")

函数公式中的`f(x)`表示我要定义一个函数，这个函数需要根据x求出结果，等号右边就是该函数的展开式，展开式就是函数的内容，定义函数的好处是简化式子，我们可以设x为一个具体的数字，把这个具体的数字带入到等号右边的`x`中进行计算，通过函数的展开式计算出最终的结果。

> 这个函数展开式里面的e是一个常数，此函数的计算原理后面会说明，也可以自行询问AI，让AI给出更详细的描述。

这个函数在函数图中的坐标如下：

![sigmoid激活函数图](./images/sigmoid-image.png "sigmoid激活函数图")

学会函数定义之后，我们自己就可以自定义一个属于自己的函数`f(x)`，或者`f(n, m)`，参数可以自己定义，内容也可以自己定义，比如`sigmoid`函数和`softmax`函数就是创造神经网络的数学家或者计算机科学家们定义的。

#### 4.1.2、微积分

搞清楚函数之后，接下来就到微积分了，微积分是一种计算方式，我们小学初中的时候，就曾经学习过圆形的面积公式，圆形的面积公式是通过想象把圆切割成无数个小三角形，越小的三角形，弧度就越接近直线，这时候将这些三角形组合成一个平行四边形，利用平行四边形的公式，就可以求出来该圆的面积，这就是积分。

这种先切割成无数简单的单元，再组合成可以简单理解的整体的思想，就是微积分思想，所以以前老师说，圆的面积公式是由微积分推导出来的，这就是微积分在生活中的其中一个应用场景。

我们生活中经常用到微积分相关的知识和思想，在生活中还有其他使用微积分的例子，比如路程公式：速度 * 时间 = 路程，这个公式也使用到了微积分的思想。只不过在现实中，我们的速度是不稳定的，可能随着时间越来越快，或者随着时间越来越慢，这种情况会在函数图像中形成一条曲线，只是我们一般只需要计算出来平均速度，而微积分可以通过对路程公式的微分和积分，计算出某个瞬间的速度（加速度），而不仅仅是总体的平均速度。

路程S = 速度v * 时间t，可以得出一个结论，路程的组成部分是速度和时间，而在微积分中，路程是无数个瞬间所走过的路程组合在一起的，也就是无数个速度碎片通过时间组成路程，这种拆分的过程叫微分，不同的时间下，加速度就不一样，组合起来就叫积分：

> 如果速度是时间的函数 v = v(t)，而s是v对时间的积分，就可以得出下列微积分公式，其中的f是微分符号，表示其中一小部分，d是积分符号，表示整体和全部的意思。

![路程时间速度函数](./images/svt-function.png "路程时间速度函数")

现在已经求出整体，但如果需要通过整体去求这个积分函数的组成部分，可以用`ds/dt = v(t)`，读作s对t的导数等于v(t)，从理解上来看，斜杠是一个除号，而导数就是我们需要寻找的答案，相当于路程除以速度等于时间，而这里的三个项都是函数，所以叫求导数:

![路程时间速度函数](./images/dsdtvs-function.png "路程时间速度函数")

#### 4.1.3 链式法则

在实际计算中，一个函数整体可能是多个函数的嵌套组成的，我们必须对一些复杂的算式进行简化，然后用简单的表达式来表示其关系，以便推导和求导，在推导完成后，再进行详细的展开计算，微积分中的**链式法则**公式，是求导的关键，如果有一个函数`y = f(g(x)) * g(x)`，其中y是x的复合函数，求y对x的导数，可以写为：

![一元函数](./images/one-function.png "一元函数")

这个公式看起来很复杂，就可以用莱布尼茨记法简写来替代函数，设u = g(x)，则y = f(u)，则：

![求导公式](./images/dao-function.png "求导公式")

所以，实际上在输入层和隐藏层之间有权重参数和偏置参数，经过加权计算，以及使用sigmoid函数计算以后，才会将计算结果保存到隐藏层，随后再经过另一个权重参数和偏置参数计算，最后将数据存储到输出层，再经过softmax计算出置信度，把所有的输出结果以百分比的小数形式显示出来。

然后整个神经网络最核心的部分其实是权重，偏置参数还有配置参数，以及运行这个神经网络所使用的算法。输入层，隐藏层，输出层，反而不是最重要的，这三个层只是用于存储计算结果，跟我认识的三层神经网络结构还是很不一样的，我的主观感受是这样的。

然后神经网络的学习是基于反向传播的，根据当前输出层的结果，以及正确答案，去计算出来实际的误差值，跟学习率和隐藏层输入的值，去计算出对应的权重参数和偏置参数所需要调整的大小，学习率用来控制调整强度，经过多轮调整后才能达到预期效果。

接下来的隐藏层和前面的权重层，以及偏置参数是如何计算调整的，具体的原理我还不理解，没办法继续了，AI跟我说是跟微积分的链式法则有关系。

我现在感觉脑子不够用了，因为我的数学知识只停留在初中，我只是会写一些代码，所以很多与代码相通的概念就比较容易理解，不过我不知道怎么理解没有答案也可以计算出隐藏层误差的情况，明天再继续想了，以上是我这个0基础没有数学知识的人所理解的内容，可能不太准确

前向传播其实是根据以下的顺序进行计算的
a → [a·w₁ + b₁] → m → sigmoid → h → [h·w₂ + b₂] → z → softmax(z) → o
在反向传播中，需要通过链式法则对当前函数的组成部分进行求导，因为函数之间的关系就是组成关系，需要找该函数的组成部分，就需要使用链式法则公式dy/dx = dy/du*du/dx，

例如：a * b = c，dc/db = a，读作c对b的偏导数等于a，dc/da = b，c对a的偏导数等于b，可以理解为，函数c是由函数a和函数b组成。

因为o函数的直接组成部分只有z，所以需要对z进行求导，z经过softmax函数的计算，所以求z的导数就是loss函数，设真实标签为y，即dL/dz = (o - y)，
因为整个网络结构都是一个完整的算式，所以可以根据链式法则，找到对应函数的组成部分，
比如h * w₂ + b₂ = z，z函数的组成部分是h和w，而此处的b在函数图像中用于平移，并不影响函数图像的形状，
也不会影响预测值在函数图像中相对于函数图像的落点，所以这里的b导数是1，但是一般不写，

根据h * w₂ + b₂ = z可以得出对应的求导表达式dz/dw₂ = h，dz/b₂ = 1，dz/dh = w₂，因为此处局部表达式的h和w₂前面没有数，所以不再分解
因为要修改w₂，可以得出dL/dw₂ = dL/dz * dz/dw₂ * 1，但是一般不写1，所以简化为dL/dw₂ = dL/dz * dz/dw₂，
因为dz/dw₂ = h，所以简化为dL/dw₂ = dL/dz * h，与学习率相乘，再赋以负号表示反方向调节，让w₂加上这个数进行调节就完成对w₂的调整，
修改b₂的式子为dL/db₂ = dL/dz * dz/db₂，因为dz/b₂ = 1，所以dL/db₂ = dL/dz，与学习率相乘，再赋以负号表示反方向调节，让b₂加上这个数进行调节就完成对b₂的调整，

对sigmoid求导得dh/dm = h*(1 - h)，根据a * w₁ + b₁ = m，得dm/dw₁ = a, dm/b₁ = 1，需要修改w₁，
所以求dL/dw₁：dL/dw₁ = dL/dh * dh/dm * dm/dw₁;
求dL/db₁: dL/db₁ = dL/dh * dh/dm，同样将结果与学习率相乘，再赋以负号调节即可。